import logging
from typing import Tuple

from openai import AsyncOpenAI

import verifiers as vf
from verifiers.types import Messages, State

ANSWERER_SYSTEM_PROMPT = """
You are playing twenty questions. You are the player who comes up with the thing to be guessed, and you chose: {answer}

The other player is trying to guess {answer} based on your answers to their yes/no questions.

YOUR JOB: Answer their questions about "{answer}" with Yes, No, or I don't know.

Do not overthink - just answer the question about {answer}

Format your response using XML tags:
<answer>Yes</answer>
<answer>No</answer>  
<answer>I don't know</answer>

Example: If asked "Is it alive?" and you're thinking of "dog", respond: <answer>Yes</answer>
"""

ANSWERER_USER_PROMPT = "{question}"

QUESTIONER_SYSTEM_PROMPT = """
You are playing twenty questions. Try to guess what the user is thinking of by asking yes or no questions. You have up to 20 questions, but may make your guess early if you wish. 
If you make your guess early, it will count as one of your 20 questions, so guess wisely!

Format your responses using XML tags:

For questions:
<question>
Your yes/no question here
</question>

To make a guess at any time use:
<guess>
Your guess here
</guess>

Remember that early guesses cost you one of your questions, but after your 20th question you will be allowed a final guess.

CRITICAL: 
- Always use either <question> or <guess>, never both in the same response.
- You MUST use proper XML formatting with both opening AND closing tags. Your response will fail to parse if tags are not properly closed.
"""

WINNING_MESSAGE = "Yes! The answer was '{answer}'. You got it!"
MAX_QUESTIONS_MESSAGE = "No! The answer was '{answer}'. You lost!"


class TwentyQuestionsEnv(vf.MultiTurnEnv):
    """
    A twenty questions environment that uses an external LLM to answer questions generated by the model:

    1. QUESTIONER LLM (being evaluated): Asks questions, tries to guess the secret
    2. ANSWERER LLM (environment): Knows the secret, answers yes/no questions
    """

    def __init__(
        self,
        answerer_client: AsyncOpenAI,
        answerer_model: str,
        **kwargs,
    ):
        super().__init__(max_turns=21, **kwargs)  # 20 questions + 1 final guess

        self.answerer_client = answerer_client
        self.answerer_model = answerer_model
        self.answerer_sampling_args = {"temperature": 0.3}

        self.answerer_system_prompt = ANSWERER_SYSTEM_PROMPT

        self.answerer_parser = vf.XMLParser(
            fields=["answer"],
            answer_field="answer",
        )

        self.logger = logging.getLogger(f"verifiers.envs.{self.__class__.__name__}")

    async def setup_state(self, state: State, **kwargs) -> State:
        """Initialize game state with the answer from the current dataset row."""
        state["questions_asked"] = 0
        state["game_over"] = False
        state["final_message_sent"] = False
        return state

    async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        """Game is complete after final message has been sent."""
        game_over = state.get("game_over", False)
        final_message_sent = state.get("final_message_sent", False)

        return game_over and final_message_sent

    async def env_response(self, messages: Messages, state: State, **kwargs) -> Tuple[Messages, State]:
        """
        Generate environment response using external LLM.
        The LLM knows the answer and answers yes/no questions or handles guesses.
        """
        if state.get("final_message_sent", False):
            state["game_over"] = True
            return [], state

        answer = state["answer"]
        questions_asked = state["questions_asked"]

        last_message_content = messages[-1]["content"]

        parsed = self.parser.parse(last_message_content)

        if hasattr(parsed, "guess") and parsed.guess:
            guess = parsed.guess.strip().lower()
            answer_lower = answer.lower()

            state["questions_asked"] = questions_asked + 1

            is_correct = False

            if guess == answer_lower:
                is_correct = True
            else:
                validation_prompt = [
                    {"role": "system", "content": self.answerer_system_prompt.format(answer=answer)},
                    {
                        "role": "user",
                        "content": f'The player guessed: "{parsed.guess}". Is this guess correct for what you\'re thinking of?',
                    },
                ]

                validation_response = await self.get_model_response(
                    client=self.answerer_client,
                    model=self.answerer_model,
                    prompt=validation_prompt,
                    sampling_args=self.answerer_sampling_args,
                    message_type="chat",
                )

                raw_validation = validation_response.choices[0].message.content or ""

                parsed_validation = self.answerer_parser.parse(raw_validation)
                if hasattr(parsed_validation, "answer") and parsed_validation.answer:
                    validation_text = parsed_validation.answer.strip().lower()
                    is_correct = validation_text.startswith("yes")
                else:
                    # Try appending </answer> and parsing again
                    fixed_validation = raw_validation + "\n</answer>"
                    parsed_validation_fixed = self.answerer_parser.parse(fixed_validation)

                    if hasattr(parsed_validation_fixed, "answer") and parsed_validation_fixed.answer:
                        validation_text = parsed_validation_fixed.answer.strip().lower()
                        is_correct = validation_text.startswith("yes")
                    else:
                        raise ValueError(f"Could not extract answer from validation response: {repr(raw_validation)}")

            if is_correct:
                state["game_won"] = True
                state["final_message_sent"] = True
                return [{"role": "user", "content": WINNING_MESSAGE.format(answer=answer)}], state
            else:
                if state["questions_asked"] >= 20:
                    state["final_message_sent"] = True
                    return [
                        {
                            "role": "user",
                            "content": MAX_QUESTIONS_MESSAGE.format(answer=answer),
                        }
                    ], state
                else:
                    return [
                        {
                            "role": "user",
                            "content": f"No, that's not correct. You have {20 - state['questions_asked']} questions left.",
                        }
                    ], state

        if not hasattr(parsed, "question") or parsed.question is None:
            # Try appending </question> and parsing again
            fixed_content = last_message_content + "\n</question>"
            parsed_fixed = self.parser.parse(fixed_content)

            if hasattr(parsed_fixed, "question") and parsed_fixed.question is not None:
                question = parsed_fixed.question.strip()
            else:
                raise ValueError(f"Could not extract question from model response: {repr(last_message_content)}")
        else:
            question = parsed.question.strip()

        answerer_prompt = [
            {"role": "system", "content": self.answerer_system_prompt.format(answer=answer)},
            {
                "role": "user",
                "content": ANSWERER_USER_PROMPT.format(question=question),
            },
        ]

        response = await self.get_model_response(
            client=self.answerer_client,
            model=self.answerer_model,
            prompt=answerer_prompt,
            sampling_args=self.answerer_sampling_args,
            message_type="chat",
        )

        raw_response = response.choices[0].message.content or ""

        parsed_answer = self.answerer_parser.parse(raw_response)

        if hasattr(parsed_answer, "answer") and parsed_answer.answer:
            env_response_text = parsed_answer.answer.strip()
        else:
            # Try appending </answer> and parsing again
            fixed_response = raw_response + "\n</answer>"
            parsed_answer_fixed = self.answerer_parser.parse(fixed_response)

            if hasattr(parsed_answer_fixed, "answer") and parsed_answer_fixed.answer:
                env_response_text = parsed_answer_fixed.answer.strip()
            else:
                raise ValueError(f"Could not extract answer from answerer response: {repr(raw_response)}")

        state["questions_asked"] = questions_asked + 1

        remaining_questions = 20 - state["questions_asked"]
        formatted_response = f"{env_response_text}\nYou have {remaining_questions} questions left."

        if state["questions_asked"] == 20:
            formatted_response = "You've used all 20 questions. Please make your final guess, what am I thinking of?"

        return [{"role": "user", "content": formatted_response}], state


def load_environment(
    answerer_model: str,
    answerer_base_url: str,
    answerer_api_key: str | None = None,
    **kwargs,
) -> TwentyQuestionsEnv:
    """
    Load the twenty questions environment.

    Args:
        answerer_model: Model name for answerer LLM (knows answers, answers questions)
        answerer_base_url: Base URL for answerer LLM API
        answerer_api_key: API key for answerer LLM (if None, uses OPENAI_API_KEY env var)

    Environment Variables:
        OPENAI_API_KEY: API key for the answerer LLM (used if answerer_api_key is None)
    """

    import os

    api_key = answerer_api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError(
            "API key is required for the answerer LLM. "
            "Either provide answerer_api_key parameter or set OPENAI_API_KEY environment variable."
        )

    answerer_client = AsyncOpenAI(
        base_url=answerer_base_url,
        api_key=api_key,
    )

    from datasets import load_dataset

    dataset = load_dataset("ljt019/twenty-questions-600")

    parser = vf.XMLParser(
        fields=["think", ("question", "guess")],
        answer_field="guess",
    )

    def victory_reward(state, **kwargs) -> float:
        """Reward for successfully guessing the answer."""
        return 1.0 if state.get("game_won", False) else 0.0

    def efficiency_reward(state, **kwargs) -> float:
        """Reward for guessing with fewer questions (efficiency bonus)."""
        if not state.get("game_won", False):
            return 0.0
        questions_used = state.get("questions_asked", 20)

        efficiency = (20 - questions_used) / 20.0
        return efficiency * 0.5

    rubric = vf.Rubric(
        funcs=[
            victory_reward,
            efficiency_reward,
            parser.get_format_reward_func(),
        ],
        weights=[1.0, 0.5, 0.3],
        parser=parser,
    )

    env = TwentyQuestionsEnv(
        dataset=dataset["train"],
        eval_dataset=dataset["test"],
        system_prompt=QUESTIONER_SYSTEM_PROMPT,
        parser=parser,
        rubric=rubric,
        answerer_client=answerer_client,
        answerer_model=answerer_model,
        **kwargs,
    )

    return env
