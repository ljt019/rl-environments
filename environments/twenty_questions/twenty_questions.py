import logging
from typing import Tuple

import httpx
from openai import AsyncOpenAI

import verifiers as vf
from verifiers.types import Messages, State


def setup_client(
    api_base_url, api_key, timeout=600.0, max_connections=100, max_keepalive_connections=50, max_retries=3
):
    timeout_obj = httpx.Timeout(timeout, connect=5.0)
    limits = httpx.Limits(max_connections=max_connections, max_keepalive_connections=max_keepalive_connections)
    http_client = httpx.AsyncClient(limits=limits, timeout=timeout_obj)
    return AsyncOpenAI(base_url=api_base_url, api_key=api_key, max_retries=max_retries, http_client=http_client)


ANSWERER_SYSTEM_PROMPT = """
You are playing twenty questions. You are the player who comes up with the thing to be guessed, and you chose: {answer}

The other player is trying to guess {answer} based on your answers to their yes/no questions.

YOUR JOB: Answer their questions about "{answer}" with Yes, No, or I don't know.

Do not overthink - just answer the question about {answer}

Expected format:
{format_str}

Examples: <answer>Yes</answer> or <answer>No</answer> or <answer>I don't know</answer>
"""

ANSWERER_USER_PROMPT = "{question}"

QUESTIONER_SYSTEM_PROMPT = """
You are playing twenty questions. Try to guess what the user is thinking of by asking yes or no questions. You have up to 20 questions, but may make your guess early if you wish. 
If you make your guess early, it will count as one of your questions, so guess wisely!

Use <think> tags to show your reasoning, then either <question> for yes/no questions or <guess> for your final answer.

Remember that early guesses cost you one of your questions, but after your 20th question you will be allowed a final guess.

CRITICAL: 
- Always use either <question> or <guess>, never both in the same response.
- You MUST use proper XML formatting with both opening AND closing tags.

Expected format:
{format_str}
"""

WINNING_MESSAGE = "Yes! The answer was '{answer}'. You got it!"
MAX_QUESTIONS_MESSAGE = "No! The answer was '{answer}'. You lost!"


class TwentyQuestionsEnv(vf.MultiTurnEnv):
    """
    A twenty questions environment that uses an external LLM to answer questions generated by the model:

    1. QUESTIONER LLM (being evaluated): Asks questions, tries to guess the secret
    2. ANSWERER LLM (environment): Knows the secret, answers yes/no questions
    """

    def __init__(
        self,
        answerer_client: AsyncOpenAI,
        answerer_model: str,
        **kwargs,
    ):
        super().__init__(max_turns=-1, **kwargs)

        self.answerer_client = answerer_client
        self.answerer_model = answerer_model
        self.answerer_sampling_args = {"temperature": 0.3}

        self.answerer_parser = vf.XMLParser(
            fields=["answer"],
            answer_field="answer",
        )

        self.answerer_system_prompt = ANSWERER_SYSTEM_PROMPT.format(
            answer="{answer}",
            format_str=self.answerer_parser.get_format_str(),
        )

        self.logger = logging.getLogger(f"verifiers.envs.{self.__class__.__name__}")

    async def setup_state(self, state: State, **kwargs) -> State:
        """Initialize game state with the answer from the current dataset row."""
        state["questions_asked"] = 0
        state["game_over"] = False
        state["final_message_sent"] = False
        return state

    async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        """Game is complete after final message has been sent."""
        game_over = state.get("game_over", False)
        final_message_sent = state.get("final_message_sent", False)

        return game_over and final_message_sent

    async def env_response(self, messages: Messages, state: State, **kwargs) -> Tuple[Messages, State]:
        """
        Generate environment response using external LLM.
        The LLM knows the answer and answers yes/no questions or handles guesses.
        """
        if state.get("final_message_sent", False):
            state["game_over"] = True
            return [], state

        answer = state["answer"]
        questions_asked = state["questions_asked"]

        last_message_content = messages[-1]["content"]

        parsed = self.parser.parse(last_message_content)

        if not parsed or (
            not (hasattr(parsed, "guess") and parsed.guess) and not (hasattr(parsed, "question") and parsed.question)
        ):
            return [
                {
                    "role": "user",
                    "content": f"Please format your response properly using the required XML tags. Expected format:\n{self.parser.get_format_str()}",
                }
            ], state

        if hasattr(parsed, "guess") and parsed.guess:
            guess = parsed.guess.strip().lower()
            answer_lower = answer.lower()

            is_correct = False

            if guess == answer_lower:
                is_correct = True
            else:
                validation_prompt = [
                    {"role": "system", "content": self.answerer_system_prompt.format(answer=answer)},
                    {
                        "role": "user",
                        "content": f'The player guessed: "{parsed.guess}". Is this guess EXACTLY correct for what you\'re thinking of?\n\nOnly answer "Yes" if the guess is essentially the same as "{answer}" - either exact match or very close synonym.\n\nEXAMPLES:\n✅ ACCEPT: "Gobi Desert" for "Gobi Desert", "Great Pyramid of Giza" for "Pyramids of Giza", "chips and cheese" for "nachos"\n❌ REJECT: "desert" for "Gobi Desert", "pyramid" for "Great Pyramid", "food" for "nachos", "car" for "Toyota Camry"\n\nPartial matches, general categories, and broader types are NOT correct - the guess must specifically identify "{answer}".',
                    },
                ]

                is_correct = False
                for attempt in range(3):
                    try:
                        validation_response = await self.get_model_response(
                            client=self.answerer_client,
                            model=self.answerer_model,
                            prompt=validation_prompt,
                            sampling_args=self.answerer_sampling_args,
                            message_type="chat",
                        )

                        if not validation_response.choices:
                            self.logger.warning("API returned empty response for validation, assuming incorrect guess")
                            is_correct = False
                            break
                        raw_validation = validation_response.choices[0].message.content or ""
                        parsed_validation = self.answerer_parser.parse(raw_validation)

                        if hasattr(parsed_validation, "answer") and parsed_validation.answer:
                            validation_text = parsed_validation.answer.strip().lower()
                            is_correct = validation_text.startswith("yes")
                            break
                        else:
                            if attempt == 2:
                                self.logger.warning(
                                    "Answerer failed to format validation response, assuming incorrect guess"
                                )
                                is_correct = False
                    except Exception as e:
                        self.logger.warning(f"Answerer API call failed (attempt {attempt + 1}): {e}")
                        if attempt == 2:
                            self.logger.warning("All answerer validation attempts failed, assuming incorrect guess")
                            is_correct = False

            state["questions_asked"] = questions_asked + 1

            if is_correct:
                state["game_won"] = True
                state["game_over"] = True
                state["final_message_sent"] = True
                return [{"role": "user", "content": WINNING_MESSAGE.format(answer=answer)}], state
            else:
                if state["questions_asked"] > 20:
                    state["game_over"] = True
                    state["final_message_sent"] = True
                    return [
                        {
                            "role": "user",
                            "content": MAX_QUESTIONS_MESSAGE.format(answer=answer),
                        }
                    ], state
                else:
                    remaining_questions = 20 - state["questions_asked"]
                    return [
                        {
                            "role": "user",
                            "content": f"No, that's not correct. You have {remaining_questions} questions left.",
                        }
                    ], state

        if not hasattr(parsed, "question") or parsed.question is None:
            return [
                {
                    "role": "user",
                    "content": "Please format your response properly using <question>your question</question> or <guess>your guess</guess> tags. Make sure to use proper XML formatting with both opening and closing tags.",
                }
            ], state

        question = parsed.question.strip()

        answerer_prompt = [
            {"role": "system", "content": self.answerer_system_prompt.format(answer=answer)},
            {
                "role": "user",
                "content": ANSWERER_USER_PROMPT.format(question=question),
            },
        ]

        env_response_text = None
        for attempt in range(3):
            try:
                response = await self.get_model_response(
                    client=self.answerer_client,
                    model=self.answerer_model,
                    prompt=answerer_prompt,
                    sampling_args=self.answerer_sampling_args,
                    message_type="chat",
                )

                raw_response = response.choices[0].message.content or ""
                parsed_answer = self.answerer_parser.parse(raw_response)

                if hasattr(parsed_answer, "answer") and parsed_answer.answer:
                    env_response_text = parsed_answer.answer.strip()
                    break
                else:
                    if attempt == 2:
                        self.logger.warning("Answerer failed to format response, using fallback")
                        env_response_text = "I'm not sure. Can you rephrase your question?"
            except Exception as e:
                self.logger.warning(f"Answerer API call failed (attempt {attempt + 1}): {e}")
                if attempt == 2:
                    self.logger.warning("All answerer API attempts failed, using fallback")
                    env_response_text = "I'm having trouble responding. Can you rephrase your question?"

        state["questions_asked"] = questions_asked + 1

        remaining_questions = 20 - state["questions_asked"]
        formatted_response = f"{env_response_text}\nYou have {remaining_questions} questions left."

        if state["questions_asked"] == 20:
            formatted_response = f"{env_response_text}\nYou've used all 20 questions. Please make your final guess, what am I thinking of?"

        return [{"role": "user", "content": formatted_response}], state


def load_environment(
    answerer_model: str,
    answerer_base_url: str,
    answerer_api_key: str | None = None,
    **kwargs,
) -> TwentyQuestionsEnv:
    """
    Load the twenty questions environment.

    Args:
        answerer_model: Model name for answerer LLM (knows answers, answers questions)
        answerer_base_url: Base URL for answerer LLM API
        answerer_api_key: API key for answerer LLM (if None, uses OPENAI_API_KEY env var)

    Environment Variables:
        OPENAI_API_KEY: API key for the answerer LLM (used if answerer_api_key is None)
    """

    import os

    api_key = answerer_api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError(
            "API key is required for the answerer LLM. "
            "Either provide answerer_api_key parameter or set OPENAI_API_KEY environment variable."
        )

    answerer_client = setup_client(
        api_base_url=answerer_base_url,
        api_key=api_key,
        timeout=600.0,
        max_connections=100,
        max_keepalive_connections=50,
        max_retries=3,
    )

    from datasets import load_dataset

    dataset = load_dataset("ljt019/twenty-questions-600")

    parser = vf.XMLParser(
        fields=["think", ("question", "guess")],
        answer_field="guess",
    )

    def victory_reward(state, **kwargs) -> float:
        """Reward for successfully guessing the answer."""
        return 1.0 if state.get("game_won", False) else 0.0

    def efficiency_reward(state, **kwargs) -> float:
        """Reward for guessing with fewer questions (efficiency bonus)."""
        if not state.get("game_won", False):
            return 0.0
        questions_used = state.get("questions_asked", 20)

        efficiency = (20 - questions_used) / 20.0
        return efficiency * 0.5

    rubric = vf.Rubric(
        funcs=[
            victory_reward,
            efficiency_reward,
            parser.get_format_reward_func(),
        ],
        weights=[1.0, 0.5, 0.3],
        parser=parser,
    )

    env = TwentyQuestionsEnv(
        dataset=dataset["train"],
        eval_dataset=dataset["test"],
        system_prompt=QUESTIONER_SYSTEM_PROMPT.format(format_str=parser.get_format_str()),
        parser=parser,
        rubric=rubric,
        answerer_client=answerer_client,
        answerer_model=answerer_model,
        **kwargs,
    )

    return env
