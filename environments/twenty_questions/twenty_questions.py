import logging
import random
from typing import Tuple

from openai import AsyncOpenAI

import verifiers as vf
from verifiers.types import Messages, State

ANSWERER_SYSTEM_PROMPT = """
You are the ANSWERER in twenty questions. You know the answer word/object that the questioner is trying to guess.

Rules:
1. Answer only "Yes", "No", or "I don't know" to questions
2. Be consistent with your answers
3. If asked to guess or for the answer directly, remind them to ask yes/no questions
4. Keep track of the answer you're thinking of throughout the conversation

The answer word/object is: {answer}

Respond only with "Yes", "No", or "I don't know" unless the question is not a yes/no question.
"""

QUESTIONER_SYSTEM_PROMPT = """
You are playing twenty questions. Try to guess what the user is thinking of by asking yes or no questions. You have up to 20 questions, but may make your guess early if you wish. 
If you make your guess early, it will count as one of your 20 questions, so guess wisely!

Format your responses as:
<think>
Your reasoning about what to ask next or what you think the answer might be.
</think>

<question>
Your yes/no question here
</question>

When you're ready to make a final guess, use:
<guess>
Your final answer here
</guess>

Always use either <question> or <guess>, never both in the same response.
"""

WINNING_MESSAGE = "Yes! The answer was '{answer}'. You got it!"
MAX_QUESTIONS_MESSAGE = " That was question {max_questions}. The answer was '{answer}'!"

ANSWERER_USER_PROMPT = """Conversation so far:
{context}

The human just asked: "{question}"

Please respond with only "Yes", "No", or "I don't know" based on the answer word '{answer}'. Be helpful but stick to yes/no answers."""


class TwentyQuestionsEnv(vf.MultiTurnEnv):
    """
    A twenty questions environment that uses an external LLM to answer questions generated by the model:

    1. QUESTIONER LLM (being evaluated): Asks questions, tries to guess the secret
    2. ANSWERER LLM (environment): Knows the secret, answers yes/no questions
    """

    def __init__(
        self,
        answerer_client: AsyncOpenAI,
        answerer_model: str,
        **kwargs,
    ):
        super().__init__(max_turns=20, **kwargs)

        self.answerer_client = answerer_client
        self.answerer_model = answerer_model
        self.answerer_sampling_args = {"temperature": 0.3}

        self.answerer_system_prompt = ANSWERER_SYSTEM_PROMPT

        self.answers = None

        self.logger = logging.getLogger(f"verifiers.envs.{self.__class__.__name__}")

    async def setup_state(self, state: State, **kwargs) -> State:
        """Initialize game state with a random answer."""
        if self.answers is None:
            self.answers = list(set(self.dataset["answer"]))

        answer = random.choice(self.answers)
        state["answer"] = answer
        state["questions_asked"] = 0
        state["game_over"] = False
        return state

    async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        """Game is complete after 20 questions or if game_over flag is set."""
        questions_asked = state.get("questions_asked", 0)
        game_over = state.get("game_over", False)

        return game_over or questions_asked >= 20

    async def env_response(self, messages: Messages, state: State, **kwargs) -> Tuple[Messages, State]:
        """
        Generate environment response using external LLM.
        The LLM knows the answer and answers yes/no questions or handles guesses.
        """
        answer = state["answer"]
        questions_asked = state["questions_asked"]

        last_message_content = messages[-1]["content"]

        parsed = self.parser.parse(last_message_content)

        if hasattr(parsed, "guess") and parsed.guess:
            guess = parsed.guess.strip().lower()
            answer_lower = answer.lower()

            state["questions_asked"] = questions_asked + 1
            state["final_guess"] = guess

            if guess == answer_lower or answer_lower in guess:
                state["game_won"] = True
                state["game_over"] = True
                return [{"role": "user", "content": WINNING_MESSAGE.format(answer=answer)}], state
            else:
                if state["questions_asked"] >= 20:
                    state["game_over"] = True
                    return [
                        {
                            "role": "user",
                            "content": f"No, that's not correct. {MAX_QUESTIONS_MESSAGE.format(max_questions=20, answer=answer)}",
                        }
                    ], state
                else:
                    return [
                        {
                            "role": "user",
                            "content": f"No, that's not correct. You have {20 - state['questions_asked']} questions left.",
                        }
                    ], state

        if hasattr(parsed, "question") and parsed.question:
            question = parsed.question.strip()
        else:
            question = last_message_content

        conversation_context = []
        if len(messages) > 6:
            conversation_context = messages[-6:]
        else:
            conversation_context = messages

        context_str = ""
        for msg in conversation_context:
            role = "Human" if msg["role"] == "assistant" else "Environment"
            context_str += f"{role}: {msg['content']}\n"

        answerer_prompt = [
            {"role": "system", "content": self.answerer_system_prompt.format(answer=answer)},
            {
                "role": "user",
                "content": ANSWERER_USER_PROMPT.format(context=context_str, question=question, answer=answer),
            },
        ]

        response = await self.get_model_response(
            client=self.answerer_client,
            model=self.answerer_model,
            prompt=answerer_prompt,
            sampling_args=self.answerer_sampling_args,
            message_type="chat",
        )

        env_response_text = response.choices[0].message.content or "I don't know"

        state["questions_asked"] = questions_asked + 1

        if state["questions_asked"] >= 20:
            env_response_text += MAX_QUESTIONS_MESSAGE.format(max_questions=20, answer=answer)
            state["game_over"] = True
            state["game_won"] = False

        return [{"role": "user", "content": env_response_text}], state


def load_environment(
    answerer_model: str,
    answerer_base_url: str,
    answerer_api_key: str | None = None,
    **kwargs,
) -> TwentyQuestionsEnv:
    """
    Load the twenty questions environment.

    Args:
        answerer_model: Model name for answerer LLM (knows answers, answers questions)
        answerer_base_url: Base URL for answerer LLM API
        answerer_api_key: API key for answerer LLM (if None, uses OPENAI_API_KEY env var)

    Environment Variables:
        OPENAI_API_KEY: API key for the answerer LLM (used if answerer_api_key is None)
    """

    import os

    api_key = answerer_api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError(
            "API key is required for the answerer LLM. "
            "Either provide answerer_api_key parameter or set OPENAI_API_KEY environment variable."
        )

    answerer_client = AsyncOpenAI(
        base_url=answerer_base_url,
        api_key=api_key,
    )

    from datasets import load_dataset

    dataset = load_dataset("ljt019/twenty-questions-600")

    parser = vf.XMLParser(
        fields=["think", ("question", "guess")],
        answer_field="guess",
    )

    def victory_reward(state: State) -> float:
        """Reward for successfully guessing the answer."""
        return 1.0 if state.get("game_won", False) else 0.0

    def efficiency_reward(state: State) -> float:
        """Reward for guessing with fewer questions (efficiency bonus)."""
        if not state.get("game_won", False):
            return 0.0
        questions_used = state.get("questions_asked", 20)

        efficiency = (20 - questions_used) / 20.0
        return efficiency * 0.5

    rubric = vf.Rubric(
        funcs=[
            victory_reward,
            efficiency_reward,
            parser.get_format_reward_func(),
        ],
        weights=[1.0, 0.5, 0.3],
        parser=parser,
    )

    env = TwentyQuestionsEnv(
        dataset=dataset["train"],
        eval_dataset=dataset["test"],
        system_prompt=QUESTIONER_SYSTEM_PROMPT,
        parser=parser,
        rubric=rubric,
        answerer_client=answerer_client,
        answerer_model=answerer_model,
        **kwargs,
    )

    return env
